train:
  seed: 42
  batch_size: 512
  max_steps: 750_000  # for 4 GPUs
  limit_val_batches: 32
  val_check_interval: 1000
  log_every_n_steps: 100
  gradient_clip_val: 1.
  lr_warmup_steps: 1000
  ema_decay: 0.9999
  optimizer:
    type: adamw
    lr: 3.e-4
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.999
  scheduler:
    type: plateau
    factor: 0.8
    patience: 5
    min_lr: 1.e-5

datasets:
  type: text8
  root: ./data/text8
  seq_len: 256

model:
  type: sphere
  data_dims: [ 256 ]
  n_class: 27
  ot: false
encoder:
  type: dit
  hidden_size: 768
  cond_dim: 128
  vocab_size: 27
  length: 256
  n_blocks: 12
  n_heads: 12
  dropout: 0.1

sample:
  n_step: 256
  n_sample: 3
