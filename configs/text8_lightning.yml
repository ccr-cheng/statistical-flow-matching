train:
  seed: 42
  batch_size: 512
  max_steps: 500_000  # for 8 GPUs
  limit_val_batches: 32
  val_check_interval: 1000
  log_every_n_steps: 100
  gradient_clip_val: 5.
  lr_warmup_steps: 1000
  ema_decay: 0.9999
  optimizer:
    type: adamw
    lr: 1.e-4
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.999
  scheduler:
    type: plateau
    factor: 0.8
    patience: 5
    min_lr: 1.e-5

datasets:
  type: text8
  root: ./data/text8
  seq_len: 256

model:
  type: sphere
  data_dims: [ 256 ]
  n_class: 27
  ot: false
encoder:
  type: gpt
  vocab_size: 27
  n_layer: 12
  n_head: 12
  n_embd: 768
  dropout: 0.0
  skip: True
  bias: True

sample:
  n_step: 256
  n_sample: 3
